from matplotlib.pyplot as plt
from sklearn.datasets import load_files
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
#Step 2: Plotting
classes=['business', 'entertainment', 'politics', 'sport', 'tech']
Instances=[510, 386, 417, 511, 401]
plt.plot(classes, Instances)
plt.xlabel("Classes")
plt.ylabel("Instances")
plt.title("BBC distribution")
print(plt.show())

#Step 3: Load files
file=load_files('C:\\Users\\manue\\Desktop\\Manuel\\COMP 472\\BBC', categories=classes, encoding='latin1')

#Step 4: Turn and fit text files into a vectorized matrix and list, respectively
vect=CountVectorizer()
X=vect.fit_transform(file.data)
y=file.target

#Step 5: Split 80% of the dataset into a training set and 20% into a test set (Both as matrices and lists)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=None)

#Step 6: Train the training dataset. Then do the same for the test dataset

#Training set
clf_train=MultinomialNB()
clf_train.fit(X_train, y_train)
clf_train.get_params(True)
clf_train.partial_fit(X_train, y_train)
clf_train.predict(X_train)
clf_train.predict_log_proba(X_train)
clf_train.predict_proba(X_train)
clf_train.score(X_train, y_train)
parameterA={'alpha': 1.0, 'class_prior': None, 'fit_prior': True}
clf_train.set_params(**parameterA)

#Test set
clf_test=MultinomialNB()
clf_test.fit(X_test, y_test)
clf_test.get_params(True)
clf_test.partial_fit(X_test, y_test)
clf_test.predict(X_test)
clf_test.predict_log_proba(X_test)
clf_test.predict_proba(X_test)
clf_test.score(X_test, y_test)
parameterB={'alpha': 1.0, 'class_prior': None, 'fit_prior': True}
clf_test.set_params(**parameterB)

#Step 7:
#a)

print("/********************MultinomialNB default values, try 1********************/")
#b)

print("Confusion matrix")
print(confusion_matrix(clf_test.predict(X_test), y_test))
print()
#c)

print("Precision, recall, and F1-measure")
classes=['business', 'entertainment', 'politics', 'sport', 'tech']
print(classification_report(y_test, clf_test.predict(X_test), target_names=classes))
print()
#d)

print("Accuracy")
print(accuracy_score(y_test, clf_test.predict(X_test)))
print("Macro average F1")
print(f1_score(y_test, clf_test.predict(X_test), average='macro'))
print("Weighted average F1")
print(f1_score(y_test, clf_test.predict(X_test), average='weighted'))
#f)

vocabulary=0
default=y_test[0]
for i in y_test:
    j=i+1
for j in y_test:
    if(default!=y_test[j]):
        vocabulary=vocabulary+1
default=y_test[i+1]
print()
print("Vocabulary size:")
print(vocabulary)
print()
#h)

total_tokens=0
for i in y_test:
    total_tokens=total_tokens+1
print()
print("Number of word-tokens in the whole corpus")
print(total_tokens)
print()

#j)
tokens1=0
for i in y_test:
    if(y_test[i]==1):
        tokens1=tokens1+1
print()
print("Number of word-tokens of frequency 1 in the whole corpus")
print(tokens1)
print("Percentage of word-tokens of frequency 1 in the whole corpus")
print(tokens1/total_tokens)
print()
#k)

print("Favorite words count and frequency")
favwords=['boost', 'search']
favwords=file.target
fwords_count=0
for i in favwords:
    for j in y_test:
        if(favwords[i]==y_test[j]):
            fwords_count=fwords_count+1
print()
print("Favorite word count in count in dataset:")
print(fwords_count)
print()